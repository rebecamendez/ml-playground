{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground 1: Predicting titanic survival using XGBoost\n",
    "\n",
    "We will use the [Titanic dataset](https://www.kaggle.com/datasets/brendan45774/test-file) to predict the survival of passengers using [XGBoost][https://es.wikipedia.org/wiki/XGBoost].\n",
    "Dataset has these relevant fields:\n",
    "- `Survived`: if passenger survived or not\n",
    "- `Pclass`: passenger class\n",
    "- `Sex`: passenger sex\n",
    "- `Age`: passenger age\n",
    "- `SibSp`: number of passenger siblings inside the ship\n",
    "- `Parch`: number of passenger parents inside the ship\n",
    "\n",
    "This is the workflow that we follow to train and validate the model:\n",
    "\n",
    "<img src=\"./diagrams.drawio.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data columns: PassengerId,Survived,Pclass,Name,Sex,Age,Siblings,Parents,Ticket,Fare,Cabin,Embarked\n",
      "data size:  (418, 12) survived: [266 152]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "data = pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "# debug\n",
    "print('data columns:', ','.join(data.columns.values))\n",
    "print('data size: ', data.shape, 'survived:', data['Survived'].value_counts().values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesing data\n",
    "we remove all the columns that are not useful for the model, drop rows with missing values and transform and encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_transformed columns: Survived,Sex,Age,Siblings,Parents,Fare,Embarked_Q,Embarked_S,Pclass_2,Pclass_3\n",
      "data_transformed size:  (331, 10)\n"
     ]
    }
   ],
   "source": [
    "data_transformed = data.copy()\n",
    "data_transformed.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True) # not useful variables\n",
    "data_transformed.dropna(how='any', inplace=True) # drop rows with missing values\n",
    "\n",
    "data_transformed['Sex'] = data_transformed['Sex'].map({'male': 0, 'female': 1})\n",
    "data_transformed = pd.get_dummies(data_transformed, columns=['Embarked', 'Pclass'], drop_first=True) # one-hot encoding for categorical variables\n",
    "\n",
    "# debug\n",
    "data_transformed.to_csv('./tmp/titanic_transformed.csv', index=False)\n",
    "print('data_transformed columns:', ','.join(data_transformed.columns.values))\n",
    "print('data_transformed size: ', data_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split dataset into training and test\n",
    "\n",
    "We need to split the data into training (80%) and testing (20%) sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size (264, 9) (264,)\n",
      "Testing size (67, 9) (67,)\n"
     ]
    }
   ],
   "source": [
    "features = data_transformed.drop('Survived', axis=1)\n",
    "objective = data_transformed['Survived']\n",
    "\n",
    "features_train, features_test, objective_train, objective_test = train_test_split(features, objective, test_size=0.20, random_state=42)\n",
    "\n",
    "# debug\n",
    "print(\"Training size\", features_train.shape, objective_train.shape)\n",
    "print(\"Testing size\", features_test.shape, objective_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model: XGBoost\n",
    "Configure and train the model. After training, we will evaluate the model with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost model trained with training accuracy 1.00\n",
      "ðŸ”Ž XGBoost model testing with training accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "model.fit(features_train, objective_train)\n",
    "\n",
    "train_accuracy = model.score(features_train, objective_train)\n",
    "test_accuracy = model.score(features_test, objective_test)\n",
    "\n",
    "\n",
    "# debug\n",
    "print(f\"âœ… XGBoost model trained with training accuracy {train_accuracy:.2f}\")\n",
    "print(f\"ðŸ”Ž XGBoost model testing with training accuracy {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Extract most relevant features\n",
    "We can also check which features are the most important for the model.\n",
    "As we expect, the most important feature is sex because all females survived. It's the reason because we have 100% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important features:\n",
      "    Feature  Importance\n",
      "0       Sex         1.0\n",
      "1       Age         0.0\n",
      "2  Siblings         0.0\n",
      "3   Parents         0.0\n",
      "4      Fare         0.0\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({'Feature': features_train.columns, 'Importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# debug\n",
    "print(\"Most important features:\")\n",
    "print(feature_importance_df.head(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make some real predictions\n",
    "Finally, let's predict the survivability of a passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survivability: 99.12%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "passenger = pd.DataFrame([{\n",
    "    'Sex': 1,\n",
    "}])\n",
    "\n",
    "# Encode data\n",
    "passenger_encoded = pd.get_dummies(passenger)\n",
    "passenger_encoded = passenger_encoded.reindex(columns=features_train.columns, fill_value=0) # ensure the columns are the same as the training data\n",
    "\n",
    "survivability = model.predict_proba(passenger_encoded)\n",
    "survivability_probability = (survivability[:, 1] * 100).item()\n",
    "\n",
    "print(\"Survivability: {:.2f}%\".format(survivability_probability))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
